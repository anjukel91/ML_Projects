{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c63d9a6-dc86-4196-9c0f-7c07d4420635",
   "metadata": {},
   "source": [
    "# NLP Preprocessing and Feature Extraction\n",
    "This notebook demonstrates text preprocessing steps:\n",
    "\n",
    "-Tokenization\n",
    "\n",
    "-Stopwords removal\n",
    "\n",
    "-Stemming\n",
    "\n",
    "-Lemmatization\n",
    "\n",
    "**We will use NLTK and spaCy libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89dce3ac-c285-4153-8933-1e9191d110cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\anju\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\anju\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\anju\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anju\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anju\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\anju\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 8.5 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.1/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.8/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.4/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 7.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 6.9 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "642017c5-5947-4ecb-8dff-7a1bae00c771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Anju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')#Downloads the tokenizer model used by word_tokenize()\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')#Downloads the list of stopwords (like the, is, in).\n",
    "nltk.download('wordnet')#Downloads the WordNet lexical database (needed for lemmatization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706565a7-96cd-491b-8983-5abb0e932364",
   "metadata": {},
   "source": [
    "### Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d261d959-58c3-4d07-b891-1cf32c5fe3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning Natural Language Processing, NLP is fascinating and powerful!\n"
     ]
    }
   ],
   "source": [
    "text=\"I am learning Natural Language Processing, NLP is fascinating and powerful!\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019dfeab-c0cf-4c63-b169-fc986e76814e",
   "metadata": {},
   "source": [
    "# 1. Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4a9664d-8496-4798-a731-645a3d44e464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', ',', 'NLP', 'is', 'fascinating', 'and', 'powerful', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29abb3-6841-4cdb-95d1-4e94a0022fe3",
   "metadata": {},
   "source": [
    "## 2. Stopwords Removal with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec0b7a4b-f77b-47cc-938b-dad071061f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learning', 'Natural', 'Language', 'Processing', ',', 'NLP', 'fascinating', 'powerful', '!']\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('English'))\n",
    "filtered_tokens=[w for w in tokens if w.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b9f79-ffc7-4d5e-ae0d-496863cd6757",
   "metadata": {},
   "source": [
    "## 3. Stemming with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f291f87b-82c7-4a0e-853f-fba11ebf1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn', 'natur', 'languag', 'process', ',', 'nlp', 'fascin', 'power', '!']\n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "stem=[stemmer.stem(word) for word in filtered_tokens]\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bc4e6-9cae-48ff-aaa4-67d5eff162f9",
   "metadata": {},
   "source": [
    "## 4. Lemmatization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cc70e31-c508-457c-9936-57c4ffabcb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learning', 'Natural', 'Language', 'Processing', ',', 'NLP', 'fascinating', 'powerful', '!']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmas=[lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3196f4-2a6f-4505-954e-d9bfafb026c3",
   "metadata": {},
   "source": [
    "Use stemming if speed is more important than accuracy.\n",
    "\n",
    "Use lemmatization if you need meaningful dictionary words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeccf75-4676-43aa-ad00-04988c3824a5",
   "metadata": {},
   "source": [
    "## 5. Tokenization and Lemmatization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7c63f92-bf6c-4063-8706-5cfcbc3e6cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning Natural Language Processing, NLP is fascinating and powerful!\n",
      "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', ',', 'NLP', 'is', 'fascinating', 'and', 'powerful', '!']\n",
      "['I', 'be', 'learn', 'Natural', 'Language', 'Processing', ',', 'NLP', 'be', 'fascinating', 'and', 'powerful', '!']\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp(text)\n",
    "print(doc)\n",
    "\n",
    "print([token.text for token in doc])\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28766ef-83dd-4ad9-a262-4ee053e94ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e6f69-4757-4ab4-929f-5940db1c121d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58be4b8a-b68f-4cd9-b1e7-42a9e54b4fdd",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d17acda7-7cbb-4640-8930-5aaab8b27329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3553232a-9ee6-47ce-86fd-b78c14112dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee38bb-4181-4057-994f-05a20bc5c9b6",
   "metadata": {},
   "source": [
    "# Sample Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed4670be-e9f6-4003-8584-5d0223d446eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Corpus:\n",
      "['I love natural language processing', 'Language models are powerful', 'I love deep learning for NLP']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Language models are powerful\",\n",
    "    \"I love deep learning for NLP\"\n",
    "]\n",
    "\n",
    "print(\"Sample Corpus:\")\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f26d4d-de38-4aa6-964f-a94c7978d146",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce309ba-d4b1-44a1-b395-8d57543f0e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e87c379a-e91c-41f3-9ebb-659b094364b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['are' 'deep' 'for' 'language' 'learning' 'love' 'models' 'natural' 'nlp'\n",
      " 'powerful' 'processing']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>deep</th>\n",
       "      <th>for</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>models</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>powerful</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  deep  for  language  learning  love  models  natural  nlp  powerful  \\\n",
       "0    0     0    0         1         0     1       0        1    0         0   \n",
       "1    1     0    0         1         0     0       1        0    0         1   \n",
       "2    0     1    1         0         1     1       0        0    1         0   \n",
       "\n",
       "   processing  \n",
       "0           1  \n",
       "1           0  \n",
       "2           0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de122ed2-174e-4036-8826-75fdc4c22425",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9e01b10-e398-4b52-9f75-7cbfaee2c1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['are' 'deep' 'for' 'language' 'learning' 'love' 'models' 'natural' 'nlp'\n",
      " 'powerful' 'processing']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>deep</th>\n",
       "      <th>for</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>models</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>powerful</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.355432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        are      deep       for  language  learning      love    models  \\\n",
       "0  0.000000  0.000000  0.000000  0.428046  0.000000  0.428046  0.000000   \n",
       "1  0.528635  0.000000  0.000000  0.402040  0.000000  0.000000  0.528635   \n",
       "2  0.000000  0.467351  0.467351  0.000000  0.467351  0.355432  0.000000   \n",
       "\n",
       "    natural       nlp  powerful  processing  \n",
       "0  0.562829  0.000000  0.000000    0.562829  \n",
       "1  0.000000  0.000000  0.528635    0.000000  \n",
       "2  0.000000  0.467351  0.000000    0.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "X_tfidf=tfidf.fit_transform(corpus)\n",
    "print(\"Feature names:\",tfidf.get_feature_names_out())\n",
    "pd.DataFrame(X_tfidf.toarray(),columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4bbb0-5492-4b66-a596-9ca1cf0016ae",
   "metadata": {},
   "source": [
    "# 5. Word Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "795e45e0-7f98-44cc-99ba-633739440d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Corpus: [['i', 'love', 'natural', 'language', 'processing'], ['language', 'models', 'are', 'powerful'], ['i', 'love', 'deep', 'learning', 'for', 'nlp']]\n",
      "Vector for word 'language':\n",
      "[-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
      " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
      " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
      "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
      "  9.9641159e-03  1.8466286e-02]\n",
      "Most similar to 'language': [('natural', 0.167039692401886), ('powerful', 0.13204392790794373), ('i', 0.1267007440328598), ('models', 0.0998455360531807), ('love', 0.042373016476631165), ('learning', 0.012442159466445446), ('processing', -0.012591080740094185), ('nlp', -0.014475272968411446), ('are', -0.0560765340924263), ('deep', -0.05974648892879486)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize each sentence\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "print(\"Tokenized Corpus:\", tokenized_corpus)\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get embedding for a word\n",
    "print(\"Vector for word 'language':\")\n",
    "print(model.wv['language'])\n",
    "\n",
    "# Find most similar words\n",
    "print(\"Most similar to 'language':\", model.wv.most_similar('language'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe71c9b-aa3e-49c6-abcd-744b0059ba45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
